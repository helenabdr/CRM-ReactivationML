{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d40635",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833253e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 1) IMPORT ALL REQUIRED LIBRARIES\n",
    "# =========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn / imblearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# xgboost, ensure xgboost is installed\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "url = 'DF_Model.csv'\n",
    "random_state = 42\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255d90a",
   "metadata": {},
   "source": [
    "# =========================================\n",
    "# 2) INTRODUCTION & MOTIVATION\n",
    "# =========================================\n",
    "\n",
    "GOAL:\n",
    " - We have transaction-level data (DF_Model.csv) describing clients.\n",
    " - Our objective: Identify which clients have the highest propensity to be \n",
    "   reactivated after 2 years without purchases.\n",
    "\n",
    "PLAN:\n",
    " 1) Read the CSV.\n",
    " 2) Explore data & note shape, columns, types.\n",
    " 3) Convert from transaction-level to client-level (one row per client).\n",
    " 4) Split into train/test sets.\n",
    " 5) Build multiple models: Logistic, Random Forest, XGBoost.\n",
    "    - We'll tune each model specifically for precision or recall via GridSearch.\n",
    " 6) Compare results on test set for precision, recall, and F1 via bar charts.\n",
    " 7) Discuss which approach is 'best' based on business needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 3) DATA LOADING\n",
    "# =========================================\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d373d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 4) AGGREGATING TO CLIENT-LEVEL\n",
    "# =========================================\n",
    "# We assume 'client_id' is the unique identifier for each client.\n",
    "# We'll create one row per client with aggregated features,\n",
    "# plus a target that is 1 if they were reactivated, 0 otherwise.\n",
    "\n",
    "df_agg = df.groupby('client_id', as_index=False).agg({\n",
    "    'region': 'first',           # first known region\n",
    "    'trade_sector': 'first',     # could also keep as numeric / transform\n",
    "    'n_employees': 'mean',\n",
    "    'economic_pot': 'mean',      # average potential\n",
    "    'eco_pot_class': 'first',\n",
    "    'risk_cat': 'first',\n",
    "    'net': 'mean',                # mean net across all transactions\n",
    "    'target': 'max',              # if any transaction had target=1 => 1\n",
    "    'item_id': 'first',\n",
    "    'flg_tool': 'first',\n",
    "    'sales_channel': 'first',\n",
    "    'family_code': 'first',\n",
    "    'n_purchases': 'max'\n",
    "})\n",
    "\n",
    "\n",
    "print(\"Aggregated Data Shape:\", df_agg.shape)\n",
    "print(\"Sample Aggregated Rows:\")\n",
    "display(df_agg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c40379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 5) TRAIN/TEST SPLIT & BASIC PREPROCESSING\n",
    "# =========================================\n",
    "\n",
    "# X = all features except client_id & target\n",
    "X = df_agg.drop(['client_id','target'], axis=1)\n",
    "y = df_agg['target']\n",
    "\n",
    "# Split data (stratify helps preserve class proportions)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# Define columns to pass through\n",
    "passthrough_cols = ['flg_tool']\n",
    "# Identify numeric vs. categorical columns\n",
    "# (You can refine these choices.)\n",
    "numeric_cols = ['n_employees','economic_pot','net','n_purchases']\n",
    "cat_cols = ['region','eco_pot_class','risk_cat','sales_channel']  # trade_sector could be included if desired\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", RobustScaler())\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipe, numeric_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols),\n",
    "    (\"passthrough\", \"passthrough\", passthrough_cols)\n",
    "])\n",
    "\n",
    "print(\"Preprocessing set up. We can now feed it into a pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 6) CLASS IMBALANCE & SCORING FOR PRECISION vs. RECALL\n",
    "# =========================================\n",
    "# We'll do two sets of GridSearch for each model:\n",
    "#  1) 'Precision' => scoring=precision_scorer\n",
    "#  2) 'Recall'    => scoring=recall_scorer\n",
    "# We'll use class_weight or scale_pos_weight to handle imbalance \n",
    "# while preserving the original data distribution.\n",
    "\n",
    "precision_scorer = make_scorer(precision_score, pos_label=1)\n",
    "recall_scorer    = make_scorer(recall_score,    pos_label=1)\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 7) MODEL 1: LOGISTIC REGRESSION\n",
    "# =========================================\n",
    "\n",
    "pipe_log = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "param_log = {\n",
    "    \"clf__C\": [0.1, 1.0, 10],\n",
    "    \"clf__class_weight\": [None, \"balanced\"]  # Could also try dict like {0:1,1:5} etc.\n",
    "}\n",
    "\n",
    "# --- (A) Optimize for PRECISION ---\n",
    "grid_log_prec = GridSearchCV(\n",
    "    estimator=pipe_log,\n",
    "    param_grid=param_log,\n",
    "    scoring=precision_scorer,\n",
    "    cv=3\n",
    ")\n",
    "grid_log_prec.fit(X_train, y_train)\n",
    "\n",
    "best_log_prec = grid_log_prec.best_estimator_\n",
    "y_pred_log_prec = best_log_prec.predict(X_test)\n",
    "\n",
    "res_log_prec = {\n",
    "    \"model\": \"Logistic(Precision)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_log_prec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_log_prec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_log_prec)\n",
    "}\n",
    "results.append(res_log_prec)\n",
    "\n",
    "# --- (B) Optimize for RECALL ---\n",
    "grid_log_rec = GridSearchCV(\n",
    "    estimator=pipe_log,\n",
    "    param_grid=param_log,\n",
    "    scoring=recall_scorer,\n",
    "    cv=3\n",
    ")\n",
    "grid_log_rec.fit(X_train, y_train)\n",
    "\n",
    "best_log_rec = grid_log_rec.best_estimator_\n",
    "y_pred_log_rec = best_log_rec.predict(X_test)\n",
    "\n",
    "res_log_rec = {\n",
    "    \"model\": \"Logistic(Recall)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_log_rec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_log_rec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_log_rec)\n",
    "}\n",
    "results.append(res_log_rec)\n",
    "\n",
    "print(\"Done training Logistic for precision & recall.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf18a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 8) MODEL 2: RANDOM FOREST\n",
    "# =========================================\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_rf = {\n",
    "    \"clf__n_estimators\": [50, 100],\n",
    "    \"clf__max_depth\": [3, 5, 10],\n",
    "    \"clf__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "# --- (A) Optimize for PRECISION ---\n",
    "grid_rf_prec = GridSearchCV(\n",
    "    pipe_rf,\n",
    "    param_rf,\n",
    "    scoring=precision_scorer,\n",
    "    cv=5\n",
    ")\n",
    "grid_rf_prec.fit(X_train, y_train)\n",
    "\n",
    "best_rf_prec = grid_rf_prec.best_estimator_\n",
    "y_pred_rf_prec = best_rf_prec.predict(X_test)\n",
    "\n",
    "res_rf_prec = {\n",
    "    \"model\": \"RF(Precision)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_rf_prec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_rf_prec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_rf_prec)\n",
    "}\n",
    "results.append(res_rf_prec)\n",
    "\n",
    "# --- (B) Optimize for RECALL ---\n",
    "grid_rf_rec = GridSearchCV(\n",
    "    pipe_rf,\n",
    "    param_rf,\n",
    "    scoring=recall_scorer,\n",
    "    cv=5\n",
    ")\n",
    "grid_rf_rec.fit(X_train, y_train)\n",
    "\n",
    "best_rf_rec = grid_rf_rec.best_estimator_\n",
    "y_pred_rf_rec = best_rf_rec.predict(X_test)\n",
    "\n",
    "res_rf_rec = {\n",
    "    \"model\": \"RF(Recall)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_rf_rec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_rf_rec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_rf_rec)\n",
    "}\n",
    "results.append(res_rf_rec)\n",
    "\n",
    "print(\"Done training Random Forest for precision & recall.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 9) MODEL 3: XGBOOST\n",
    "# =========================================\n",
    "# We'll use scale_pos_weight for imbalance.\n",
    "\n",
    "pipe_xgb = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "param_xgb = {\n",
    "    \"clf__n_estimators\": [50, 100],\n",
    "    \"clf__max_depth\": [3, 5],\n",
    "    \"clf__scale_pos_weight\": [1, 5, 10]  # pick some approximate ratio or small range\n",
    "}\n",
    "\n",
    "# --- (A) Optimize for PRECISION ---\n",
    "grid_xgb_prec = GridSearchCV(\n",
    "    pipe_xgb,\n",
    "    param_xgb,\n",
    "    scoring=precision_scorer,\n",
    "    cv=5\n",
    ")\n",
    "grid_xgb_prec.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_prec = grid_xgb_prec.best_estimator_\n",
    "y_pred_xgb_prec = best_xgb_prec.predict(X_test)\n",
    "\n",
    "res_xgb_prec = {\n",
    "    \"model\": \"XGB(Precision)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_xgb_prec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_xgb_prec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_xgb_prec)\n",
    "}\n",
    "results.append(res_xgb_prec)\n",
    "\n",
    "# --- (B) Optimize for RECALL ---\n",
    "grid_xgb_rec = GridSearchCV(\n",
    "    pipe_xgb,\n",
    "    param_xgb,\n",
    "    scoring=recall_scorer,\n",
    "    cv=5\n",
    ")\n",
    "grid_xgb_rec.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_rec = grid_xgb_rec.best_estimator_\n",
    "y_pred_xgb_rec = best_xgb_rec.predict(X_test)\n",
    "\n",
    "res_xgb_rec = {\n",
    "    \"model\": \"XGB(Recall)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_xgb_rec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_xgb_rec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_xgb_rec)\n",
    "}\n",
    "results.append(res_xgb_rec)\n",
    "\n",
    "print(\"Done training XGBoost for precision & recall.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 10) COLLECT & VIEW RESULTS\n",
    "# =========================================\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"model\").reset_index(drop=True)\n",
    "print(\"FINAL COMPARISON (Test Set):\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10985290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 11) PLOT SIMPLE BAR CHARTS\n",
    "# =========================================\n",
    "models = results_df['model'].values\n",
    "precisions = results_df['precision'].values\n",
    "recalls = results_df['recall'].values\n",
    "f1s = results_df['f1'].values\n",
    "\n",
    "# -- Bar Chart for Precision --\n",
    "plt.figure()\n",
    "plt.bar(models, precisions)\n",
    "plt.title('Precision by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Precision')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -- Bar Chart for Recall --\n",
    "plt.figure()\n",
    "plt.bar(models, recalls)\n",
    "plt.title('Recall by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Recall')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -- Bar Chart for F1 --\n",
    "plt.figure()\n",
    "plt.bar(models, f1s)\n",
    "plt.title('F1 Score by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b128c",
   "metadata": {},
   "source": [
    "## ‚úÖ 12) EXPLANATION & CONCLUSION\n",
    "\n",
    "---\n",
    "\n",
    "### üîç INTERPRETING THE RESULTS:\n",
    "\n",
    "- We evaluated **6 total models**:\n",
    "\n",
    "  1. **Logistic(Precision)**  \n",
    "  2. **Logistic(Recall)**  \n",
    "  3. **Random Forest (RF) - Precision**  \n",
    "  4. **Random Forest (RF) - Recall**  \n",
    "  5. **XGBoost (XGB) - Precision**  \n",
    "  6. **XGBoost (XGB) - Recall**\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Summary of Model Performance\n",
    "\n",
    "| Model                | Precision | Recall   | F1 Score |\n",
    "|---------------------|-----------|----------|----------|\n",
    "| Logistic(Precision) | 0.235     | 0.001    | 0.003    |\n",
    "| Logistic(Recall)    | 0.194     | 0.640    | 0.298    |\n",
    "| RF(Precision)        | 0.253     | 0.890    | 0.394    |\n",
    "| RF(Recall)           | 0.221     | 0.928    | 0.357    |\n",
    "| XGB(Precision)       | **0.552** | 0.104    | 0.174    |\n",
    "| XGB(Recall)          | 0.255     | **0.944**| **0.402** |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Insights & Recommendations:\n",
    "\n",
    "- **Logistic Regression**, while interpretable, struggles on this problem‚Äîespecially when tuned for precision (F1 ‚âà 0.003).\n",
    "- **Random Forest** and **XGBoost** outperform Logistic in **every metric**, particularly on recall-tuned versions.\n",
    "- **XGB(Recall)** is the best overall model based on **F1 Score (0.402)** and **Recall (0.944)**‚Äîideal if catching as many returners as possible is the goal.\n",
    "- **XGB(Precision)** stands out with a **precision of 0.552**, useful if **minimizing false positives** is more important.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Business Application Guidance:\n",
    "\n",
    "- ‚úÖ **Choose XGB(Recall)** if your goal is **not to miss potential reactivations** (max coverage).\n",
    "- ‚úÖ **Choose XGB(Precision)** if you **only want highly likely returners**, avoiding wasted outreach.\n",
    "- ‚öñÔ∏è Use **F1-score or PR-AUC** when you want a **balanced strategy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac657f",
   "metadata": {},
   "source": [
    "#### üèÜ Best Models by Objective\n",
    "\n",
    "| Goal                | Best Model     | Why                                           |\n",
    "|---------------------|----------------|-----------------------------------------------|\n",
    "| **High Precision**  | XGB(Precision) | Highest precision `0.552`                     |\n",
    "| **High Recall**     | XGB(Recall)    | Highest recall `0.944`                        |\n",
    "| **Balanced (F1)**   | XGB(Recall)    | Highest F1 score `0.402`                      |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ What to Do Next\n",
    "\n",
    "#### 1. **Threshold Tuning**\n",
    "- Especially for **XGB(Recall)**: raise the decision threshold (> 0.5) to reduce false positives.\n",
    "- This could **boost precision** and increase the overall F1-score further.\n",
    "\n",
    "#### 2. **Feature Engineering**\n",
    "- Despite strong recall, all models have moderate precision ‚Äî false positives are still a concern.\n",
    "- Add richer features like:\n",
    "  - **Time since last purchase**\n",
    "  - **Interaction terms** (e.g., sector √ó region)\n",
    "  - **RFM features** (Recency, Frequency, Monetary value)\n",
    "\n",
    "#### 3. **Model Calibration**\n",
    "- Use **CalibratedClassifierCV** to produce better probability estimates.\n",
    "- Helps refine thresholds more reliably across different business contexts.\n",
    "\n",
    "#### 4. **XGB Optimization**\n",
    "- While XGB performed well overall, there's room to tune further:\n",
    "  - Try `eval_metric='aucpr'`\n",
    "  - Adjust `scale_pos_weight` to handle imbalance more effectively\n",
    "  - Use **early stopping** on validation sets\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Final Thoughts\n",
    "\n",
    "- ‚úÖ **XGBoost (Recall)** is the top pick for high coverage and balance ‚Äî ideal for reactivation-focused strategies.\n",
    "- ‚úÖ **XGBoost (Precision)** is ideal when **every outreach has a cost**.\n",
    "- üîÑ **Random Forest** remains a strong, robust baseline.\n",
    "- ‚ö†Ô∏è **Logistic Regression** is limited on this dataset ‚Äî suitable more for interpretation than raw performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445cf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace previous param_xgb with a more robust grid\n",
    "param_xgb_fixed = {\n",
    "    \"clf__max_depth\": [3, 5, 7],\n",
    "    \"clf__learning_rate\": [0.01, 0.1],\n",
    "    \"clf__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"clf__colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Retry precision-optimized XGBoost\n",
    "grid_xgb_prec_fixed = GridSearchCV(\n",
    "    estimator=pipe_xgb,\n",
    "    param_grid=param_xgb_fixed,\n",
    "    scoring=precision_scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_xgb_prec_fixed.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_prec_fixed = grid_xgb_prec_fixed.best_estimator_\n",
    "y_pred_xgb_prec_fixed = best_xgb_prec_fixed.predict(X_test)\n",
    "\n",
    "res_xgb_prec_fixed = {\n",
    "    \"model\": \"XGB(Precision*)\",  # mark as improved\n",
    "    \"precision\": precision_score(y_test, y_pred_xgb_prec_fixed),\n",
    "    \"recall\":    recall_score(y_test, y_pred_xgb_prec_fixed),\n",
    "    \"f1\":        f1_score(y_test, y_pred_xgb_prec_fixed)\n",
    "}\n",
    "results.append(res_xgb_prec_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for positive class\n",
    "y_probs = best_xgb_rec.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Sweep thresholds from 0.1 to 0.9\n",
    "thresholds = np.arange(0.1, 0.91, 0.05)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_probs >= t).astype(int)\n",
    "    precision_scores.append(precision_score(y_test, y_pred_thresh))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_thresh))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(thresholds, precision_scores, label=\"Precision\")\n",
    "plt.plot(thresholds, recall_scores, label=\"Recall\")\n",
    "plt.plot(thresholds, f1_scores, label=\"F1 Score\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"XGB(Recall) - Threshold Tuning\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b917f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Extract the trained XGBoost model from the pipeline\n",
    "xgb_model = best_xgb_rec.named_steps['clf']\n",
    "\n",
    "# Transform the test data using the preprocessor\n",
    "X_test_transformed = best_xgb_rec.named_steps['prep'].transform(X_test)\n",
    "\n",
    "# Initialize SHAP TreeExplainer with the XGBoost model\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "\n",
    "# Compute SHAP values for the transformed test data\n",
    "shap_values = explainer.shap_values(X_test_transformed)\n",
    "\n",
    "# Retrieve feature names after preprocessing\n",
    "feature_names = best_xgb_rec.named_steps['prep'].get_feature_names_out()\n",
    "\n",
    "# Generate a SHAP summary plot for feature importance\n",
    "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ede89",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = best_xgb_rec.named_steps['clf']\n",
    "# For 'gain' importance\n",
    "importance_dict = xgb_model.get_booster().get_score(importance_type='cover')\n",
    "feature_names = best_xgb_rec.named_steps['prep'].get_feature_names_out()\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': [importance_dict.get(f, 0) for f in feature_names]\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Plot the top 10 features\n",
    "importance_df.head(10).plot(kind='barh', x='Feature', y='Importance', legend=False)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e0dd2",
   "metadata": {},
   "source": [
    "This will show three curves:\n",
    "- **Precision**: how \"clean\" the predictions are (fewer false positives)\n",
    "- **Recall**: how many true positives you captured\n",
    "- **F1 Score**: the harmonic mean (balance) of precision and recall\n",
    "\n",
    "---\n",
    "\n",
    "## üìä How to Read the Graph\n",
    "\n",
    "- **X-axis**: classification **threshold** (from 0.1 to 0.9)\n",
    "- **Y-axis**: score for each metric (0‚Äì1)\n",
    "- You‚Äôll see:\n",
    "  - As threshold increases:\n",
    "    - **Precision goes up** (you only take high-confidence clients)\n",
    "    - **Recall goes down** (you miss many true reactivations)\n",
    "    - **F1 score** peaks somewhere in the middle\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Example Interpretation:\n",
    "\n",
    "| Threshold | Precision | Recall | F1      | Meaning                                             |\n",
    "|-----------|-----------|--------|---------|-----------------------------------------------------|\n",
    "| 0.25      | 0.11      | 0.92   | 0.20    | Very aggressive: catches many, but lots of false pos |\n",
    "| 0.50      | 0.16      | 0.84   | 0.27    | Default value; moderate recall/precision             |\n",
    "| 0.75      | 0.30      | 0.40   | 0.34    | More cautious; better quality, fewer reactivations   |\n",
    "| 0.90      | 0.60      | 0.10   | 0.17    | Very conservative; only the most confident clients   |\n",
    "\n",
    "You pick a **threshold that fits your strategy**:\n",
    "- Want more leads? Pick a **low threshold** ‚Üí high recall.\n",
    "- Want only high-quality leads? Pick a **high threshold** ‚Üí high precision.\n",
    "- Want a balanced trade-off? Pick the **threshold with highest F1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c684540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best threshold based on tradeoff\n",
    "best_thresh = 0.7  # for example\n",
    "y_pred_adjusted = (y_probs >= best_thresh).astype(int)\n",
    "\n",
    "adjusted_result = {\n",
    "    \"model\": f\"XGB(Recall_Tuned@{best_thresh})\",\n",
    "    \"precision\": precision_score(y_test, y_pred_adjusted),\n",
    "    \"recall\": recall_score(y_test, y_pred_adjusted),\n",
    "    \"f1\": f1_score(y_test, y_pred_adjusted)\n",
    "}\n",
    "results.append(adjusted_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae60308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create updated comparison DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"f1\", ascending=False)\n",
    "\n",
    "# Plot new scores\n",
    "for metric in [\"precision\", \"recall\", \"f1\"]:\n",
    "    plt.figure()\n",
    "    plt.bar(results_df[\"model\"], results_df[metric])\n",
    "    plt.title(f\"{metric.capitalize()} by Model\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1b69e",
   "metadata": {},
   "source": [
    "## üìä How to Read Each Graph\n",
    "\n",
    "### üî∑ Precision by Model\n",
    "\n",
    "- **X-axis**: Each bar represents a model.\n",
    "- **Y-axis**: How many of the predicted ‚Äúreactivated‚Äù clients were actually reactivated.\n",
    "- **Higher is better** *only* if false positives are expensive for you (e.g. costly to contact clients who won‚Äôt convert).\n",
    "\n",
    "‚úÖ Use this graph if your business wants to:\n",
    "> Focus effort only where you're highly confident the client will reactivate.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Recall by Model\n",
    "\n",
    "- **X-axis**: Each model again.\n",
    "- **Y-axis**: Out of all true reactivatable clients, how many did the model catch?\n",
    "- **Higher is better** if **missing opportunities is costly**, even if it means you get some wrong ones too.\n",
    "\n",
    "‚úÖ Use this graph if your business wants to:\n",
    "> Maximize how many real reactivations you catch (you can tolerate contacting some wrong clients).\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ F1 Score by Model\n",
    "\n",
    "- **X-axis**: Models.\n",
    "- **Y-axis**: F1 score = harmonic mean of precision and recall.\n",
    "- This is a **balanced metric**, useful when you want a **trade-off** between catching many real cases and not wasting resources.\n",
    "\n",
    "‚úÖ Use this if:\n",
    "> You want a well-rounded model that performs ‚Äúsolidly‚Äù across both goals.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Example Use-Case Summary\n",
    "\n",
    "| Graph               | Use it when...                                                 | What to look for                          |\n",
    "|---------------------|----------------------------------------------------------------|--------------------------------------------|\n",
    "| Precision by Model  | You want **quality > quantity** of predictions                | Look for highest bars on this chart        |\n",
    "| Recall by Model     | You want to catch **as many reactivatable clients as possible** | Focus on models with highest recall bars   |\n",
    "| F1 Score by Model   | You want a **balanced decision-making tool**                   | Pick the best overall-performing model     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b94260",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"model\").reset_index(drop=True)\n",
    "print(\"FINAL COMPARISON (Test Set):\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942551d2",
   "metadata": {},
   "source": [
    "### üîç By the end\n",
    "\n",
    "We now have **8 models** including:\n",
    "\n",
    "- The original 6 (Logistic, RF, XGB √ó Precision/Recall)\n",
    "- Plus:\n",
    "  - ‚úÖ `XGB(Precision*)`: retrained for precision\n",
    "  - ‚úÖ `XGB(Recall_Tuned@0.7)`: recall-optimized + threshold tuning at 0.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
