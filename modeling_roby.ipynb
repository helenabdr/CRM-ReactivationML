{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d40635",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833253e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 1) IMPORT ALL REQUIRED LIBRARIES\n",
    "# =========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn / imblearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# xgboost, ensure xgboost is installed\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "url = '/Users/Roby/MLFieldWork/DF_Model.csv'\n",
    "random_state = 42\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255d90a",
   "metadata": {},
   "source": [
    "# =========================================\n",
    "# 2) INTRODUCTION & MOTIVATION\n",
    "# =========================================\n",
    "\n",
    "GOAL:\n",
    " - We have transaction-level data (DF_Model.csv) describing clients.\n",
    " - Our objective: Identify which clients have the highest propensity to be \n",
    "   reactivated after 2 years without purchases.\n",
    "\n",
    "PLAN:\n",
    " 1) Read the CSV.\n",
    " 2) Explore data & note shape, columns, types.\n",
    " 3) Convert from transaction-level to client-level (one row per client).\n",
    " 4) Split into train/test sets.\n",
    " 5) Build multiple models: Logistic, Random Forest, XGBoost.\n",
    "    - We'll tune each model specifically for precision or recall via GridSearch.\n",
    " 6) Compare results on test set for precision, recall, and F1 via bar charts.\n",
    " 7) Discuss which approach is 'best' based on business needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 3) DATA LOADING\n",
    "# =========================================\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d373d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 4) AGGREGATING TO CLIENT-LEVEL\n",
    "# =========================================\n",
    "# We assume 'client_id' is the unique identifier for each client.\n",
    "# We'll create one row per client with aggregated features,\n",
    "# plus a target that is 1 if they were reactivated, 0 otherwise.\n",
    "\n",
    "df_agg = df.groupby('client_id', as_index=False).agg({\n",
    "    'region': 'first',           # first known region\n",
    "    'trade_sector': 'first',     # could also keep as numeric / transform\n",
    "    'n_employees': 'mean',\n",
    "    'economic_pot': 'mean',      # average potential\n",
    "    'eco_pot_class': 'first',\n",
    "    'risk_cat': 'first',\n",
    "    'net': 'mean',                # mean net across all transactions\n",
    "    'target': 'max'              # if any transaction had target=1 => 1\n",
    "})\n",
    "\n",
    "\n",
    "print(\"Aggregated Data Shape:\", df_agg.shape)\n",
    "print(\"Sample Aggregated Rows:\")\n",
    "display(df_agg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 5) TRAIN/TEST SPLIT & BASIC PREPROCESSING\n",
    "# =========================================\n",
    "\n",
    "# X = all features except client_id & target\n",
    "X = df_agg.drop(['client_id','target'], axis=1)\n",
    "y = df_agg['target']\n",
    "\n",
    "# Split data (stratify helps preserve class proportions)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# Identify numeric vs. categorical columns\n",
    "# (You can refine these choices.)\n",
    "numeric_cols = ['n_employees','economic_pot','net']\n",
    "cat_cols = ['region','eco_pot_class','risk_cat']  # trade_sector could be included if desired\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipe, numeric_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "print(\"Preprocessing set up. We can now feed it into a pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 6) CLASS IMBALANCE & SCORING FOR PRECISION vs. RECALL\n",
    "# =========================================\n",
    "# We'll do two sets of GridSearch for each model:\n",
    "#  1) 'Precision' => scoring=precision_scorer\n",
    "#  2) 'Recall'    => scoring=recall_scorer\n",
    "# We'll use class_weight or scale_pos_weight to handle imbalance \n",
    "# while preserving the original data distribution.\n",
    "\n",
    "precision_scorer = make_scorer(precision_score, pos_label=1)\n",
    "recall_scorer    = make_scorer(recall_score,    pos_label=1)\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 7) MODEL 1: LOGISTIC REGRESSION\n",
    "# =========================================\n",
    "\n",
    "pipe_log = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "param_log = {\n",
    "    \"clf__C\": [0.1, 1.0, 10],\n",
    "    \"clf__class_weight\": [None, \"balanced\"]  # Could also try dict like {0:1,1:5} etc.\n",
    "}\n",
    "\n",
    "# --- (A) Optimize for PRECISION ---\n",
    "grid_log_prec = GridSearchCV(\n",
    "    estimator=pipe_log,\n",
    "    param_grid=param_log,\n",
    "    scoring=precision_scorer,\n",
    "    cv=3\n",
    ")\n",
    "grid_log_prec.fit(X_train, y_train)\n",
    "\n",
    "best_log_prec = grid_log_prec.best_estimator_\n",
    "y_pred_log_prec = best_log_prec.predict(X_test)\n",
    "\n",
    "res_log_prec = {\n",
    "    \"model\": \"Logistic(Precision)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_log_prec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_log_prec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_log_prec)\n",
    "}\n",
    "results.append(res_log_prec)\n",
    "\n",
    "# --- (B) Optimize for RECALL ---\n",
    "grid_log_rec = GridSearchCV(\n",
    "    estimator=pipe_log,\n",
    "    param_grid=param_log,\n",
    "    scoring=recall_scorer,\n",
    "    cv=3\n",
    ")\n",
    "grid_log_rec.fit(X_train, y_train)\n",
    "\n",
    "best_log_rec = grid_log_rec.best_estimator_\n",
    "y_pred_log_rec = best_log_rec.predict(X_test)\n",
    "\n",
    "res_log_rec = {\n",
    "    \"model\": \"Logistic(Recall)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_log_rec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_log_rec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_log_rec)\n",
    "}\n",
    "results.append(res_log_rec)\n",
    "\n",
    "print(\"Done training Logistic for precision & recall.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf18a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 8) MODEL 2: RANDOM FOREST\n",
    "# =========================================\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_rf = {\n",
    "    \"clf__n_estimators\": [50, 100],\n",
    "    \"clf__max_depth\": [3, 5, 10],\n",
    "    \"clf__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "# --- (A) Optimize for PRECISION ---\n",
    "grid_rf_prec = GridSearchCV(\n",
    "    pipe_rf,\n",
    "    param_rf,\n",
    "    scoring=precision_scorer,\n",
    "    cv=3\n",
    ")\n",
    "grid_rf_prec.fit(X_train, y_train)\n",
    "\n",
    "best_rf_prec = grid_rf_prec.best_estimator_\n",
    "y_pred_rf_prec = best_rf_prec.predict(X_test)\n",
    "\n",
    "res_rf_prec = {\n",
    "    \"model\": \"RF(Precision)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_rf_prec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_rf_prec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_rf_prec)\n",
    "}\n",
    "results.append(res_rf_prec)\n",
    "\n",
    "# --- (B) Optimize for RECALL ---\n",
    "grid_rf_rec = GridSearchCV(\n",
    "    pipe_rf,\n",
    "    param_rf,\n",
    "    scoring=recall_scorer,\n",
    "    cv=3\n",
    ")\n",
    "grid_rf_rec.fit(X_train, y_train)\n",
    "\n",
    "best_rf_rec = grid_rf_rec.best_estimator_\n",
    "y_pred_rf_rec = best_rf_rec.predict(X_test)\n",
    "\n",
    "res_rf_rec = {\n",
    "    \"model\": \"RF(Recall)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_rf_rec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_rf_rec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_rf_rec)\n",
    "}\n",
    "results.append(res_rf_rec)\n",
    "\n",
    "print(\"Done training Random Forest for precision & recall.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 9) MODEL 3: XGBOOST\n",
    "# =========================================\n",
    "# We'll use scale_pos_weight for imbalance.\n",
    "\n",
    "pipe_xgb = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "param_xgb = {\n",
    "    \"clf__n_estimators\": [50, 100],\n",
    "    \"clf__max_depth\": [3, 5],\n",
    "    \"clf__scale_pos_weight\": [1, 5, 10]  # pick some approximate ratio or small range\n",
    "}\n",
    "\n",
    "# --- (A) Optimize for PRECISION ---\n",
    "grid_xgb_prec = GridSearchCV(\n",
    "    pipe_xgb,\n",
    "    param_xgb,\n",
    "    scoring=precision_scorer,\n",
    "    cv=3\n",
    ")\n",
    "grid_xgb_prec.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_prec = grid_xgb_prec.best_estimator_\n",
    "y_pred_xgb_prec = best_xgb_prec.predict(X_test)\n",
    "\n",
    "res_xgb_prec = {\n",
    "    \"model\": \"XGB(Precision)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_xgb_prec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_xgb_prec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_xgb_prec)\n",
    "}\n",
    "results.append(res_xgb_prec)\n",
    "\n",
    "# --- (B) Optimize for RECALL ---\n",
    "grid_xgb_rec = GridSearchCV(\n",
    "    pipe_xgb,\n",
    "    param_xgb,\n",
    "    scoring=recall_scorer,\n",
    "    cv=3\n",
    ")\n",
    "grid_xgb_rec.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_rec = grid_xgb_rec.best_estimator_\n",
    "y_pred_xgb_rec = best_xgb_rec.predict(X_test)\n",
    "\n",
    "res_xgb_rec = {\n",
    "    \"model\": \"XGB(Recall)\",\n",
    "    \"precision\": precision_score(y_test, y_pred_xgb_rec),\n",
    "    \"recall\":    recall_score(y_test, y_pred_xgb_rec),\n",
    "    \"f1\":        f1_score(y_test, y_pred_xgb_rec)\n",
    "}\n",
    "results.append(res_xgb_rec)\n",
    "\n",
    "print(\"Done training XGBoost for precision & recall.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 10) COLLECT & VIEW RESULTS\n",
    "# =========================================\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"model\").reset_index(drop=True)\n",
    "print(\"FINAL COMPARISON (Test Set):\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10985290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 11) PLOT SIMPLE BAR CHARTS\n",
    "# =========================================\n",
    "models = results_df['model'].values\n",
    "precisions = results_df['precision'].values\n",
    "recalls = results_df['recall'].values\n",
    "f1s = results_df['f1'].values\n",
    "\n",
    "# -- Bar Chart for Precision --\n",
    "plt.figure()\n",
    "plt.bar(models, precisions)\n",
    "plt.title('Precision by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Precision')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -- Bar Chart for Recall --\n",
    "plt.figure()\n",
    "plt.bar(models, recalls)\n",
    "plt.title('Recall by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Recall')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -- Bar Chart for F1 --\n",
    "plt.figure()\n",
    "plt.bar(models, f1s)\n",
    "plt.title('F1 Score by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b128c",
   "metadata": {},
   "source": [
    "# =========================================\n",
    "# 12) EXPLANATION & CONCLUSION\n",
    "# =========================================\n",
    " \n",
    "INTERPRETING THE RESULTS:\n",
    " - We have 6 total models: \n",
    "    1) Logistic(Precision),  2) Logistic(Recall),\n",
    "    3) RF(Precision),        4) RF(Recall),\n",
    "    5) XGB(Precision),       6) XGB(Recall).\n",
    " \n",
    " - Models tuned for Precision generally yield higher precision (fewer false positives)\n",
    "   but lower recall (more false negatives).\n",
    " - Models tuned for Recall generally yield higher recall but often lower precision.\n",
    " \n",
    " - Which approach is 'best' depends on business goals. If it's critical not to miss\n",
    "   potential reactivations, choose a high-recall model. If it's expensive to contact\n",
    "   false-positives, choose a high-precision model. \n",
    " - F1 or PR-AUC can provide a balanced perspective if you value both equally.\n",
    " \n",
    " - Random Forest / XGBoost often perform better than Logistic Regression on complex data,\n",
    "   but Logistic is simpler and can be more interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac657f",
   "metadata": {},
   "source": [
    "### ✅ Evaluation Summary\n",
    "\n",
    "We trained and evaluated six models — two for each algorithm (Logistic Regression, Random Forest, XGBoost), optimizing one version for **precision**, and the other for **recall**. Here's a breakdown of how each model performed on the **test set**:\n",
    "\n",
    "| Model               | Precision | Recall   | F1       |\n",
    "|---------------------|-----------|----------|----------|\n",
    "| Logistic(Precision) | 0.191     | 0.577    | 0.287    |\n",
    "| Logistic(Recall)    | 0.189     | 0.578    | 0.285    |\n",
    "| RF(Precision)       | 0.216     | 0.522    | **0.306**|\n",
    "| RF(Recall)          | 0.215     | 0.521    | 0.304    |\n",
    "| XGB(Precision)      | 0.000     | 0.000    | 0.000    |\n",
    "| XGB(Recall)         | 0.157     | **0.840**| 0.265    |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Interpretation & Comments\n",
    "\n",
    "#### 🔍 General Observations\n",
    "\n",
    "- The **best overall F1 score** was achieved by **Random Forest (Precision)**: `F1 = 0.306`.\n",
    "- **XGBoost (Precision)** completely failed (`precision = recall = f1 = 0.000`), likely due to:\n",
    "  - Poor parameter choice (e.g. `scale_pos_weight` too high or irrelevant features).\n",
    "  - An overfitting or underfitting issue during training.\n",
    "- **XGBoost (Recall)** got an extremely **high recall** (`0.84`), meaning it successfully captured most positive cases but sacrificed precision.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🏆 Best Models by Objective\n",
    "\n",
    "| Goal                | Best Model        | Why                                           |\n",
    "|---------------------|-------------------|-----------------------------------------------|\n",
    "| **High Precision**  | RF(Precision)     | Highest precision `0.216` + best F1 `0.306`   |\n",
    "| **High Recall**     | XGB(Recall)       | Highest recall `0.84`                         |\n",
    "| **Balanced (F1)**   | RF(Precision)     | Best balance of precision and recall          |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 What to Do Next\n",
    "\n",
    "#### 1. **Fix XGBoost(Precision)**\n",
    "- XGB’s failure in precision-optimization suggests it may not have learned anything useful.\n",
    "- Try tuning `scale_pos_weight`, or even test XGBoost with `eval_metric=\"aucpr\"` and early stopping.\n",
    "\n",
    "#### 2. **Consider Threshold Tuning**\n",
    "- Especially for **XGB(Recall)**: you can reduce false positives by increasing the decision threshold (default = 0.5).\n",
    "- This might improve its **precision**, and increase the F1 to match or beat Random Forest.\n",
    "\n",
    "#### 3. **Feature Engineering**\n",
    "- All models show precision < 0.22 → false positives are a concern.\n",
    "- Consider adding:\n",
    "  - Time-based features (e.g. last purchase gap)\n",
    "  - Interaction terms (region × sector, risk × economic potential)\n",
    "  - Frequency/monetary variables per client (RFM features)\n",
    "\n",
    "#### 4. **Calibration**\n",
    "- Try using **calibrated probabilities** (e.g. with `CalibratedClassifierCV`) to make better threshold decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Final Thoughts\n",
    "\n",
    "- Random Forest is your **most stable and robust performer**.\n",
    "- XGBoost can give exceptional **recall**, but requires care (and currently needs fixing for precision).\n",
    "- Logistic Regression performs reasonably, but struggles with non-linearity — consistent with expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445cf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace previous param_xgb with a more robust grid\n",
    "param_xgb_fixed = {\n",
    "    \"clf__n_estimators\": [100, 200],\n",
    "    \"clf__max_depth\": [3, 5, 7],\n",
    "    \"clf__learning_rate\": [0.01, 0.1],\n",
    "    \"clf__scale_pos_weight\": [1, 3, 5, 10]\n",
    "}\n",
    "\n",
    "# Retry precision-optimized XGBoost\n",
    "grid_xgb_prec_fixed = GridSearchCV(\n",
    "    estimator=pipe_xgb,\n",
    "    param_grid=param_xgb_fixed,\n",
    "    scoring=precision_scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_xgb_prec_fixed.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_prec_fixed = grid_xgb_prec_fixed.best_estimator_\n",
    "y_pred_xgb_prec_fixed = best_xgb_prec_fixed.predict(X_test)\n",
    "\n",
    "res_xgb_prec_fixed = {\n",
    "    \"model\": \"XGB(Precision*)\",  # mark as improved\n",
    "    \"precision\": precision_score(y_test, y_pred_xgb_prec_fixed),\n",
    "    \"recall\":    recall_score(y_test, y_pred_xgb_prec_fixed),\n",
    "    \"f1\":        f1_score(y_test, y_pred_xgb_prec_fixed)\n",
    "}\n",
    "results.append(res_xgb_prec_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for positive class\n",
    "y_probs = best_xgb_rec.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Sweep thresholds from 0.1 to 0.9\n",
    "thresholds = np.arange(0.1, 0.91, 0.05)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_probs >= t).astype(int)\n",
    "    precision_scores.append(precision_score(y_test, y_pred_thresh))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_thresh))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(thresholds, precision_scores, label=\"Precision\")\n",
    "plt.plot(thresholds, recall_scores, label=\"Recall\")\n",
    "plt.plot(thresholds, f1_scores, label=\"F1 Score\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"XGB(Recall) - Threshold Tuning\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e0dd2",
   "metadata": {},
   "source": [
    "This will show three curves:\n",
    "- **Precision**: how \"clean\" the predictions are (fewer false positives)\n",
    "- **Recall**: how many true positives you captured\n",
    "- **F1 Score**: the harmonic mean (balance) of precision and recall\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 How to Read the Graph\n",
    "\n",
    "- **X-axis**: classification **threshold** (from 0.1 to 0.9)\n",
    "- **Y-axis**: score for each metric (0–1)\n",
    "- You’ll see:\n",
    "  - As threshold increases:\n",
    "    - **Precision goes up** (you only take high-confidence clients)\n",
    "    - **Recall goes down** (you miss many true reactivations)\n",
    "    - **F1 score** peaks somewhere in the middle\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Example Interpretation:\n",
    "\n",
    "| Threshold | Precision | Recall | F1      | Meaning                                             |\n",
    "|-----------|-----------|--------|---------|-----------------------------------------------------|\n",
    "| 0.25      | 0.11      | 0.92   | 0.20    | Very aggressive: catches many, but lots of false pos |\n",
    "| 0.50      | 0.16      | 0.84   | 0.27    | Default value; moderate recall/precision             |\n",
    "| 0.75      | 0.30      | 0.40   | 0.34    | More cautious; better quality, fewer reactivations   |\n",
    "| 0.90      | 0.60      | 0.10   | 0.17    | Very conservative; only the most confident clients   |\n",
    "\n",
    "You pick a **threshold that fits your strategy**:\n",
    "- Want more leads? Pick a **low threshold** → high recall.\n",
    "- Want only high-quality leads? Pick a **high threshold** → high precision.\n",
    "- Want a balanced trade-off? Pick the **threshold with highest F1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c684540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best threshold based on tradeoff\n",
    "best_thresh = 0.45  # for example\n",
    "y_pred_adjusted = (y_probs >= best_thresh).astype(int)\n",
    "\n",
    "adjusted_result = {\n",
    "    \"model\": f\"XGB(Recall_Tuned@{best_thresh})\",\n",
    "    \"precision\": precision_score(y_test, y_pred_adjusted),\n",
    "    \"recall\": recall_score(y_test, y_pred_adjusted),\n",
    "    \"f1\": f1_score(y_test, y_pred_adjusted)\n",
    "}\n",
    "results.append(adjusted_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae60308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create updated comparison DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"f1\", ascending=False)\n",
    "\n",
    "# Plot new scores\n",
    "for metric in [\"precision\", \"recall\", \"f1\"]:\n",
    "    plt.figure()\n",
    "    plt.bar(results_df[\"model\"], results_df[metric])\n",
    "    plt.title(f\"{metric.capitalize()} by Model\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1b69e",
   "metadata": {},
   "source": [
    "## 📊 How to Read Each Graph\n",
    "\n",
    "### 🔷 Precision by Model\n",
    "\n",
    "- **X-axis**: Each bar represents a model.\n",
    "- **Y-axis**: How many of the predicted “reactivated” clients were actually reactivated.\n",
    "- **Higher is better** *only* if false positives are expensive for you (e.g. costly to contact clients who won’t convert).\n",
    "\n",
    "✅ Use this graph if your business wants to:\n",
    "> Focus effort only where you're highly confident the client will reactivate.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔷 Recall by Model\n",
    "\n",
    "- **X-axis**: Each model again.\n",
    "- **Y-axis**: Out of all true reactivatable clients, how many did the model catch?\n",
    "- **Higher is better** if **missing opportunities is costly**, even if it means you get some wrong ones too.\n",
    "\n",
    "✅ Use this graph if your business wants to:\n",
    "> Maximize how many real reactivations you catch (you can tolerate contacting some wrong clients).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔷 F1 Score by Model\n",
    "\n",
    "- **X-axis**: Models.\n",
    "- **Y-axis**: F1 score = harmonic mean of precision and recall.\n",
    "- This is a **balanced metric**, useful when you want a **trade-off** between catching many real cases and not wasting resources.\n",
    "\n",
    "✅ Use this if:\n",
    "> You want a well-rounded model that performs “solidly” across both goals.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Example Use-Case Summary\n",
    "\n",
    "| Graph               | Use it when...                                                 | What to look for                          |\n",
    "|---------------------|----------------------------------------------------------------|--------------------------------------------|\n",
    "| Precision by Model  | You want **quality > quantity** of predictions                | Look for highest bars on this chart        |\n",
    "| Recall by Model     | You want to catch **as many reactivatable clients as possible** | Focus on models with highest recall bars   |\n",
    "| F1 Score by Model   | You want a **balanced decision-making tool**                   | Pick the best overall-performing model     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b94260",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"model\").reset_index(drop=True)\n",
    "print(\"FINAL COMPARISON (Test Set):\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942551d2",
   "metadata": {},
   "source": [
    "### 🔍 Interpretation of New Results\n",
    "\n",
    "We now have **8 models** including:\n",
    "\n",
    "- The original 6 (Logistic, RF, XGB × Precision/Recall)\n",
    "- Plus:\n",
    "  - ✅ `XGB(Precision*)`: retrained for precision\n",
    "  - ✅ `XGB(Recall_Tuned@0.45)`: recall-optimized + threshold tuning at 0.45\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Updated Results Summary\n",
    "\n",
    "| Model                  | Precision | Recall   | F1       |\n",
    "|------------------------|-----------|----------|----------|\n",
    "| **RF(Precision)**      | **0.216** | 0.522    | **0.306**|\n",
    "| RF(Recall)             | 0.215     | 0.521    | 0.304    |\n",
    "| Logistic(Precision)    | 0.191     | 0.577    | 0.287    |\n",
    "| Logistic(Recall)       | 0.189     | 0.578    | 0.285    |\n",
    "| **XGB(Recall)**        | 0.157     | 0.840    | 0.265    |\n",
    "| **XGB(Recall_Tuned@0.45)** | 0.151 | **0.889**| 0.257    |\n",
    "| XGB(Precision)         | 0.000     | 0.000    | 0.000    |\n",
    "| XGB(Precision*)        | 0.000     | 0.000    | 0.000    |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Key Takeaways\n",
    "\n",
    "#### ✅ 1. **Best All-Around Model**\n",
    "- **Random Forest (Precision)** still leads overall in **F1 (0.306)** and has the best **precision** (0.216).\n",
    "- This is a balanced choice if you care about catching good clients *without too many false positives*.\n",
    "\n",
    "#### 📈 2. **High Recall Performer**\n",
    "- **XGB(Recall_Tuned@0.45)** achieved **excellent recall** at `0.889`, which means it's catching almost 9 out of 10 real reactivation cases.\n",
    "- Precision is **low** at `0.151`, so **many false positives**.\n",
    "- Use this **if missing a reactivatable client is very costly**.\n",
    "\n",
    "#### ⚠️ 3. **XGB Precision Failure**\n",
    "- Both `XGB(Precision)` and `XGB(Precision*)` failed completely.\n",
    "- Likely causes:\n",
    "  - Poor feature interaction for XGB\n",
    "  - Model collapse due to misfit or misconfiguration (e.g. bad `scale_pos_weight`, tree depth, or constant labels)\n",
    "- Consider dropping XGB for precision optimization or debugging with a simplified dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Business-Driven Decision\n",
    "\n",
    "| Business Goal                            | Recommended Model           | Why                                               |\n",
    "|------------------------------------------|-----------------------------|----------------------------------------------------|\n",
    "| Maximize reactivation recall (catch all) | `XGB(Recall_Tuned@0.45)`    | Best recall (0.889), very few missed opportunities |\n",
    "| Best balance of precision & recall       | `RF(Precision)`             | Best F1 (0.306), good precision (0.216)            |\n",
    "| Precision-focused strategy               | `RF(Precision)`             | Best available precision + overall performance     |\n",
    "| Avoid XGBoost for now                    | ✖ `XGB(Precision*)`         | Doesn't learn under current settings               |\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Recommendations\n",
    "\n",
    "1. **Use `RF(Precision)`** as the default model for now.\n",
    "2. Deploy **`XGB(Recall_Tuned@0.45)`** only for high-risk segments, or as part of a second-tier filter.\n",
    "3. Skip or re-tune `XGB(Precision)` from scratch (check data imbalance, labels, params).\n",
    "4. Consider combining both approaches in a **two-stage model**:\n",
    "   - Stage 1: High-recall model for broad filtering\n",
    "   - Stage 2: High-precision model for final decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98ac50",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
