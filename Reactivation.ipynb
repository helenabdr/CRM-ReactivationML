{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "\n",
    "client = 'TOOL_CLIENT.csv'\n",
    "sales  = 'TOOL_SALES.csv'\n",
    "joint = 'TOOL_JOINT.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client = pd.read_csv(client)\n",
    "df_sales = pd.read_csv(sales)\n",
    "\n",
    "df = pd.merge(df_client, df_sales, on='CLIENT_ID')\n",
    "\n",
    "# df = pd.read_csv(joint)\n",
    "\n",
    "# lower case and replace spaces with underscores in column names\n",
    "original_columns = df.columns\n",
    "renamed_columns = [col.lower().replace(\" \", \"_\") for col in original_columns]\n",
    "column_mapping = dict(zip(original_columns, renamed_columns))\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date columns to datetime\n",
    "df['client_create_date'] = pd.to_datetime(df['client_create_date'])\n",
    "df['yyyymm'] = pd.to_datetime(df['yyyymm'].astype(str), format='%Y%m')\n",
    "\n",
    "# converting other columns to appropriate data types\n",
    "df['client_id'] = df['client_id'].astype(str)\n",
    "df['cancelled'] = df['cancelled'] == 'X'\n",
    "df['unit'] = df['unit'] == 'P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the merged data\n",
    "\n",
    "# df.to_csv('TOOL_JOINT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new variable to identify client\n",
    "\n",
    "Create a new variable to identify unique sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column for identifie the unique sales\n",
    "df['sales_id'] = df['client_id'].astype(str) + '_' + df['yyyymm'].dt.strftime('%Y%m')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new dataframes with no canceled orders\n",
    "no_canceled = df[df['cancelled'] == False]\n",
    "no_canceled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique sales per client\n",
    "client_sales_count = no_canceled.groupby('client_id')['sales_id'].nunique().sort_values(ascending=False)\n",
    "print(client_sales_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_sales_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the clients with more than one sale\n",
    "client_sales_count.drop(client_sales_count[client_sales_count < 2].index, inplace=True)\n",
    "\n",
    "pluri_client = no_canceled[no_canceled['client_id'].isin(client_sales_count.index)]\n",
    "pluri_client['n_purchases'] = pluri_client.groupby('client_id')['sales_id'].transform('nunique')\n",
    "\n",
    "print(pluri_client.shape)\n",
    "print(pluri_client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pluri_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in pluri_client.columns:\n",
    "    print(c)\n",
    "    print(pluri_client[c].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a new dataset cointaining only client with more than one purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.countplot(x='n_purchases', data=pluri_client, hue='unit')\n",
    "plt.xlabel('Number of Purchases')\n",
    "plt.ylabel('Number of Clients')\n",
    "plt.title('Number of Purchases per Client')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this point I'd like asking business about articles that aren't \"Unit\". I suppose that could be substitute parts for other articles or similar, which a client buys because it absolutly needs it and a marketing campaign about is unuseful. So I decide to drop rows containing non-piece units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unnecessary columns\n",
    "pluri_client.drop(pluri_client[pluri_client['unit'] == False].index, inplace=True)\n",
    "pluri_client.drop('unit', axis=1, inplace=True)\n",
    "pluri_client.drop('cancelled', axis=1, inplace=True)\n",
    "\n",
    "pluri_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pluri_client.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps: \n",
    "- Days since last purchase for same client\n",
    "- Analyze time between purchases\n",
    "\n",
    "- Create a DF with total amount for each sales_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting new variables\n",
    "sales_net = pluri_client.groupby('sales_id')['net'].sum()   # total sales per sale\n",
    "sales_n_purchases = pluri_client.groupby('sales_id')['n_purchases'].max()   # number of purchases per sale\n",
    "\n",
    "sales_time = pluri_client[['sales_id', 'client_id', 'yyyymm']].drop_duplicates().set_index('client_id')   # time of the sale\n",
    "sales_time = sales_time.sort_values(by=['client_id', 'yyyymm'])\n",
    "\n",
    "sales_time['time_diff'] = sales_time.groupby('client_id')['yyyymm'].diff().dt.days  # time between sales\n",
    "sales_id_time_diff = sales_time[['sales_id', 'time_diff']].set_index('sales_id')    # time between sales per sale (to easly merge with the main dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the new variables to the main dataframe\n",
    "sales_time = pluri_client[['sales_id', 'client_id', 'yyyymm']].drop_duplicates().set_index('client_id')   # time of the sale\n",
    "sales_time = sales_time.sort_values(by=['client_id', 'yyyymm'])\n",
    "\n",
    "sales_time['time_diff'] = sales_time.groupby('client_id')['yyyymm'].diff().dt.days  # time between sales\n",
    "sales_id_time_diff = sales_time[['sales_id', 'time_diff']].set_index('sales_id')    # time between sales per sale (to easly merge with the main dataframe)\n",
    "\n",
    "pluri_client['sales_net'] = pluri_client['sales_id'].map(sales_net)\n",
    "pluri_client['time_diff'] = pluri_client['sales_id'].map(sales_id_time_diff['time_diff'].to_dict())\n",
    "pluri_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_time.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new dataframe for sales\n",
    "sales_net = sales_net.reset_index()\n",
    "sales_n_purchases = sales_n_purchases.reset_index()\n",
    "sales_time.reset_index(inplace=True)\n",
    "\n",
    "merged = pd.merge(sales_time, sales_net, on='sales_id')\n",
    "sales = pd.merge(merged, sales_n_purchases, on='sales_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the new variables to the sales dataframe\n",
    "sales_columns = ['region', 'trade_sector', 'n_employees', 'economic_pot',\n",
    "                 'eco_pot_class', 'risk_cat', 'flg_tool', 'sales_channel', 'sales_id']  # columns to be added to the sales dataframe\n",
    "\n",
    "for c in sales_columns: # adding the columns to the sales dataframe\n",
    "    col = pluri_client.groupby('sales_id')[c].first()\n",
    "    sales = pd.merge(sales, col, left_on='sales_id', right_index=True)\n",
    "\n",
    "sales.drop(['sales_id_x', 'sales_id_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. de-comment the follow cell to save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.to_csv('sales.csv', index=False)\n",
    "pluri_client.to_csv('TOOLS_PREP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 DataFrame to use for our analysis\n",
    "- df: the complete DataFrame with all the variables\n",
    "- sales: the DataFrame with the details of sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding the itemsets for the apriori algorithm\n",
    "itemsets_series = pluri_client.groupby('client_id')['item_id'].apply(list)\n",
    "encoder = TransactionEncoder()\n",
    "itemsets = itemsets_series.tolist()\n",
    "\n",
    "onehot = encoder.fit_transform(itemsets, sparse=True)\n",
    "onehot = pd.DataFrame.sparse.from_spmatrix(onehot, columns = encoder.columns_)\n",
    "onehot.columns = [str(col) for col in onehot.columns]\n",
    "onehot.info()\n",
    "\n",
    "frequent_itemsets = apriori(onehot, min_support=0.01, max_len = 4, use_colnames=True, low_memory=True)\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.2)\n",
    "print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering and ordering the rules\n",
    "rules.sort_values(by=['support', 'confidence'], ascending=False, inplace=True)\n",
    "print('We have a total of {} rules'.format(rules.shape[0]))\n",
    "rules.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
